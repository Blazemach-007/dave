{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# run this cell to import nltk\n",
      "\n",
      "import nltk\n",
      "\n",
      "from os import getcwd\n",
      "\n",
      "import w1_unittest\n",
      "\n",
      "\n",
      "\n",
      "nltk.download('twitter_samples')\n",
      "\n",
      "nltk.download('stopwords')\n",
      "\n",
      "filePath = f\"{getcwd()}/../tmp2/\"\n",
      "\n",
      "nltk.data.path.append(filePath)\n",
      "\n",
      "import numpy as np\n",
      "\n",
      "import pandas as pd\n",
      "\n",
      "from nltk.corpus import twitter_samples \n",
      "\n",
      "\n",
      "\n",
      "from tweetPrepro_freqGenerator import process_tweet, build_freqs\n",
      "\n",
      "# select the set of positive and negative tweets\n",
      "\n",
      "all_positive_tweets = twitter_samples.strings('positive_tweets.json')\n",
      "\n",
      "all_negative_tweets = twitter_samples.strings('negative_tweets.json')\n",
      "\n",
      "# split the data into two pieces, one for training and one for testing (validation set) \n",
      "\n",
      "test_pos = all_positive_tweets[4000:]\n",
      "\n",
      "train_pos = all_positive_tweets[:4000]\n",
      "\n",
      "test_neg = all_negative_tweets[4000:]\n",
      "\n",
      "train_neg = all_negative_tweets[:4000]\n",
      "\n",
      "\n",
      "\n",
      "train_x = train_pos + train_neg \n",
      "\n",
      "test_x = test_pos + test_neg\n",
      "\n",
      "# combine positive and negative labels\n",
      "\n",
      "train_y = np.append(np.ones((len(train_pos), 1)), np.zeros((len(train_neg), 1)), axis=0)\n",
      "\n",
      "test_y = np.append(np.ones((len(test_pos), 1)), np.zeros((len(test_neg), 1)), axis=0)\n",
      "\n",
      "# Print the shape train and test sets\n",
      "\n",
      "print(\"train_y.shape = \" + str(train_y.shape))\n",
      "\n",
      "print(\"test_y.shape = \" + str(test_y.shape))\n",
      "\n",
      "# create frequency dictionary\n",
      "\n",
      "freqs = build_freqs(train_x, train_y)\n",
      "\n",
      "\n",
      "\n",
      "# check the output\n",
      "\n",
      "print(\"type(freqs) = \" + str(type(freqs)))\n",
      "\n",
      "print(\"len(freqs) = \" + str(len(freqs.keys())))\n",
      "\n",
      "# test the function below\n",
      "\n",
      "print('This is an example of a positive tweet: \\n', train_x[0])\n",
      "\n",
      "print('\\nThis is an example of the processed version of the tweet: \\n', process_tweet(train_x[0]))\n",
      "\n",
      "# UNQ_C1 GRADED FUNCTION: sigmoid\n",
      "\n",
      "def sigmoid(z): \n",
      "\n",
      "    '''\n",
      "\n",
      "    Input:\n",
      "\n",
      "        z: is the input (can be a scalar or an array)\n",
      "\n",
      "    Output:\n",
      "\n",
      "        h: the sigmoid of z\n",
      "\n",
      "    '''\n",
      "\n",
      "\n",
      "\n",
      "    h = 1 / (1 + np.exp(-z))\n",
      "\n",
      "\n",
      "\n",
      "    \n",
      "\n",
      "    return h\n",
      "\n",
      "# Testing your function \n",
      "\n",
      "if (sigmoid(0) == 0.5):\n",
      "\n",
      "    print('SUCCESS!')\n",
      "\n",
      "else:\n",
      "\n",
      "    print('Oops!')\n",
      "\n",
      "\n",
      "\n",
      "if (sigmoid(4.92) == 0.9927537604041685):\n",
      "\n",
      "    print('CORRECT!')\n",
      "\n",
      "else:\n",
      "\n",
      "    print('Oops again!')\n",
      "\n",
      "# Test your function\n",
      "\n",
      "w1_unittest.test_sigmoid(sigmoid)\n",
      "\n",
      "# verify that when the model predicts close to 1, but the actual label is 0, the loss is a large positive value\n",
      "\n",
      "-1 * (1 - 0) * np.log(1 - 0.9999) # loss is about 9.2\n",
      "\n",
      "# verify that when the model predicts close to 0 but the actual label is 1, the loss is a large positive value\n",
      "\n",
      "-1 * np.log(0.0001) # loss is about 9.2\n",
      "\n",
      "# UNQ_C2 GRADED FUNCTION: gradientDescent\n",
      "\n",
      "def gradientDescent(x, y, theta, alpha, num_iters):\n",
      "\n",
      "    '''\n",
      "\n",
      "    Input:\n",
      "\n",
      "        x: matrix of features which is (m,n+1)\n",
      "\n",
      "        y: corresponding labels of the input matrix x, dimensions (m,1)\n",
      "\n",
      "        theta: weight vector of dimension (n+1,1)\n",
      "\n",
      "        alpha: learning rate\n",
      "\n",
      "        num_iters: number of iterations you want to train your model for\n",
      "\n",
      "    Output:\n",
      "\n",
      "        J: the final cost\n",
      "\n",
      "        theta: your final weight vector\n",
      "\n",
      "    Hint: you might want to print the cost to make sure that it is going down.\n",
      "\n",
      "    '''\n",
      "\n",
      "    ### START CODE HERE ###\n",
      "\n",
      "    # get 'm', the number of rows in matrix x\n",
      "\n",
      "    m = x.shape[0]     \n",
      "\n",
      "    for i in range(0, num_iters):\n",
      "\n",
      "        \n",
      "\n",
      "        # get z, the dot product of x and theta\n",
      "\n",
      "        z = np.dot(x,theta)\n",
      "\n",
      "        \n",
      "\n",
      "        # get the sigmoid of h\n",
      "\n",
      "        h = sigmoid(z)\n",
      "\n",
      "        \n",
      "\n",
      "        # calculate the cost function\n",
      "\n",
      "        J = -1./m * (np.dot(y.transpose(), np.log(h)) + np.dot((1-y).transpose(),np.log(1-h)))                                                    \n",
      "\n",
      "\n",
      "\n",
      "        # update the weights theta\n",
      "\n",
      "        theta = theta - (alpha/m) * np.dot(x.transpose(),(h-y))\n",
      "\n",
      "        \n",
      "\n",
      "    ### END CODE HERE ###\n",
      "\n",
      "    J = float(J)\n",
      "\n",
      "    return J, theta\n",
      "\n",
      "# Check the function\n",
      "\n",
      "# Construct a synthetic test case using numpy PRNG functions\n",
      "\n",
      "np.random.seed(1)\n",
      "\n",
      "# X input is 10 x 3 with ones for the bias terms\n",
      "\n",
      "tmp_X = np.append(np.ones((10, 1)), np.random.rand(10, 2) * 2000, axis=1)\n",
      "\n",
      "# Y Labels are 10 x 1\n",
      "\n",
      "tmp_Y = (np.random.rand(10, 1) > 0.35).astype(float)\n",
      "\n",
      "\n",
      "\n",
      "# Apply gradient descent\n",
      "\n",
      "tmp_J, tmp_theta = gradientDescent(tmp_X, tmp_Y, np.zeros((3, 1)), 1e-8, 700)\n",
      "\n",
      "print(f\"The cost after training is {tmp_J:.8f}.\")\n",
      "\n",
      "print(f\"The resulting vector of weights is {[round(t, 8) for t in np.squeeze(tmp_theta)]}\")\n",
      "\n",
      "# Test your function\n",
      "\n",
      "w1_unittest.test_gradientDescent(gradientDescent)\n",
      "\n",
      "# UNQ_C3 GRADED FUNCTION: extract_features\n",
      "\n",
      "def extract_features(tweet, freqs, process_tweet=process_tweet):\n",
      "\n",
      "    '''\n",
      "\n",
      "    Input: \n",
      "\n",
      "        tweet: a list of words for one tweet\n",
      "\n",
      "        freqs: a dictionary corresponding to the frequencies of each tuple (word, label)\n",
      "\n",
      "    Output: \n",
      "\n",
      "        x: a feature vector of dimension (1,3)\n",
      "\n",
      "    '''\n",
      "\n",
      "    # process_tweet tokenizes, stems, and removes stopwords\n",
      "\n",
      "    word_l = process_tweet(tweet)\n",
      "\n",
      "    \n",
      "\n",
      "    # 3 elements in the form of a 1 x 3 vector\n",
      "\n",
      "    x = np.zeros((1, 3)) \n",
      "\n",
      "    \n",
      "\n",
      "    #bias term is set to 1\n",
      "\n",
      "    x[0,0] = 1 \n",
      "\n",
      "    \n",
      "\n",
      "    ### START CODE HERE ###\n",
      "\n",
      "    \n",
      "\n",
      "    # loop through each word in the list of words\n",
      "\n",
      "    # loop through each word in the list of words\n",
      "\n",
      "    for word in word_l:\n",
      "\n",
      "        \n",
      "\n",
      "        # increment the word count for the positive label 1\n",
      "\n",
      "        x[0,1] += freqs.get((word, 1.0),0)\n",
      "\n",
      "        \n",
      "\n",
      "        # increment the word count for the negative label 0\n",
      "\n",
      "        x[0,2] += freqs.get((word, 0.0),0)\n",
      "\n",
      "        \n",
      "\n",
      "    ### END CODE HERE ###\n",
      "\n",
      "    assert(x.shape == (1, 3))\n",
      "\n",
      "    return x\n",
      "\n",
      "# Check your function\n",
      "\n",
      "# test 1\n",
      "\n",
      "# test on training data\n",
      "\n",
      "tmp1 = extract_features(train_x[0], freqs)\n",
      "\n",
      "print(tmp1)\n",
      "\n",
      "# test 2:\n",
      "\n",
      "# check for when the words are not in the freqs dictionary\n",
      "\n",
      "tmp2 = extract_features('blorb bleeeeb bloooob', freqs)\n",
      "\n",
      "print(tmp2)\n",
      "\n",
      "# Test your function\n",
      "\n",
      "w1_unittest.test_extract_features(extract_features, freqs)\n",
      "\n",
      "# collect the features 'x' and stack them into a matrix 'X'\n",
      "\n",
      "X = np.zeros((len(train_x), 3))\n",
      "\n",
      "for i in range(len(train_x)):\n",
      "\n",
      "    X[i, :]= extract_features(train_x[i], freqs)\n",
      "\n",
      "\n",
      "\n",
      "# training labels corresponding to X\n",
      "\n",
      "Y = train_y\n",
      "\n",
      "\n",
      "\n",
      "# Apply gradient descent\n",
      "\n",
      "J, theta = gradientDescent(X, Y, np.zeros((3, 1)), 1e-9, 1500)\n",
      "\n",
      "print(f\"The cost after training is {J:.8f}.\")\n",
      "\n",
      "print(f\"The resulting vector of weights is {[round(t, 8) for t in np.squeeze(theta)]}\")\n",
      "\n",
      "# UNQ_C4 GRADED FUNCTION: predict_tweet\n",
      "\n",
      "def predict_tweet(tweet, freqs, theta):\n",
      "\n",
      "    '''\n",
      "\n",
      "    Input: \n",
      "\n",
      "        tweet: a string\n",
      "\n",
      "        freqs: a dictionary corresponding to the frequencies of each tuple (word, label)\n",
      "\n",
      "        theta: (3,1) vector of weights\n",
      "\n",
      "    Output: \n",
      "\n",
      "        y_pred: the probability of a tweet being positive or negative\n",
      "\n",
      "    '''\n",
      "\n",
      "    ### START CODE HERE ###\n",
      "\n",
      "    \n",
      "\n",
      "    # extract the features of the tweet and store it into x\n",
      "\n",
      "    x = extract_features(tweet,freqs)\n",
      "\n",
      "    \n",
      "\n",
      "    # make the prediction using x and theta\n",
      "\n",
      "    y_pred = sigmoid(np.dot(x,theta))\n",
      "\n",
      "    \n",
      "\n",
      "    ### END CODE HERE ###\n",
      "\n",
      "    \n",
      "\n",
      "    return y_pred\n",
      "\n",
      "# Run this cell to test your function\n",
      "\n",
      "for tweet in ['I am happy', 'I am bad', 'this movie should have been great.', 'great', 'great great', 'great great great', 'great great great great']:\n",
      "\n",
      "    print( '%s -> %f' % (tweet, predict_tweet(tweet, freqs, theta)))    \n",
      "\n",
      "    \n",
      "\n",
      "# Feel free to check the sentiment of your own tweet below\n",
      "\n",
      "my_tweet = 'I am learning :)'\n",
      "\n",
      "predict_tweet(my_tweet, freqs, theta)\n",
      "\n",
      "# Test your function\n",
      "\n",
      "w1_unittest.test_predict_tweet(predict_tweet, freqs, theta)\n",
      "\n",
      "# UNQ_C5 GRADED FUNCTION: test_logistic_regression\n",
      "\n",
      "def test_logistic_regression(test_x, test_y, freqs, theta, predict_tweet=predict_tweet):\n",
      "\n",
      "    \"\"\"\n",
      "\n",
      "    Input: \n",
      "\n",
      "        test_x: a list of tweets\n",
      "\n",
      "        test_y: (m, 1) vector with the corresponding labels for the list of tweets\n",
      "\n",
      "        freqs: a dictionary with the frequency of each pair (or tuple)\n",
      "\n",
      "        theta: weight vector of dimension (3, 1)\n",
      "\n",
      "    Output: \n",
      "\n",
      "        accuracy: (# of tweets classified correctly) / (total # of tweets)\n",
      "\n",
      "    \"\"\"\n",
      "\n",
      "    \n",
      "\n",
      "    ### START CODE HERE ###\n",
      "\n",
      "    \n",
      "\n",
      "    # the list for storing predictions\n",
      "\n",
      "    y_hat = []\n",
      "\n",
      "    \n",
      "\n",
      "    for tweet in test_x:\n",
      "\n",
      "        # get the label prediction for the tweet\n",
      "\n",
      "        y_pred = predict_tweet(tweet, freqs, theta)\n",
      "\n",
      "        if y_pred > 0.5:\n",
      "\n",
      "            # append 1.0 to the list\n",
      "\n",
      "            y_hat.append(1)\n",
      "\n",
      "        else:\n",
      "\n",
      "            # append 0 to the list\n",
      "\n",
      "            y_hat.append(0)\n",
      "\n",
      "\n",
      "\n",
      "    # With the above implementation, y_hat is a list, but test_y is (m,1) array\n",
      "\n",
      "    # convert both to one-dimensional arrays in order to compare them using the '==' operator\n",
      "\n",
      "    \n",
      "\n",
      "    accuracy = (y_hat==np.squeeze(test_y)).sum()/len(test_x)\n",
      "\n",
      "    \n",
      "\n",
      "    ### END CODE HERE ###\n",
      "\n",
      "    \n",
      "\n",
      "    return accuracy\n",
      "\n",
      "tmp_accuracy = test_logistic_regression(test_x, test_y, freqs, theta)\n",
      "\n",
      "print(f\"Logistic regression model's accuracy = {tmp_accuracy:.4f}\")\n",
      "\n",
      "# Test your function\n",
      "\n",
      "w1_unittest.unittest_test_logistic_regression(test_logistic_regression, freqs, theta)\n",
      "\n",
      "# Some error analysis done for you\n",
      "\n",
      "print('Label Predicted Tweet')\n",
      "\n",
      "for x,y in zip(test_x,test_y):\n",
      "\n",
      "    y_hat = predict_tweet(x, freqs, theta)\n",
      "\n",
      "\n",
      "\n",
      "    if np.abs(y - (y_hat > 0.5)) > 0:\n",
      "\n",
      "        print('THE TWEET IS:', x)\n",
      "\n",
      "        print('THE PROCESSED TWEET IS:', process_tweet(x))\n",
      "\n",
      "        print('%d\\t%0.8f\\t%s' % (y, y_hat, ' '.join(process_tweet(x)).encode('ascii', 'ignore')))\n",
      "\n",
      "# Feel free to change the tweet below\n",
      "\n",
      "my_tweet = 'This is a ridiculously bright movie. The plot was terrible and I was sad until the ending!'\n",
      "\n",
      "print(process_tweet(my_tweet))\n",
      "\n",
      "y_hat = predict_tweet(my_tweet, freqs, theta)\n",
      "\n",
      "print(y_hat)\n",
      "\n",
      "if y_hat > 0.5:\n",
      "\n",
      "    print('Positive sentiment')\n",
      "\n",
      "else: \n",
      "\n",
      "    print('Negative sentiment')\n",
      "\n",
      "\n",
      "\n",
      "import numpy as np # Library for linear algebra and math utils\n",
      "\n",
      "import pandas as pd # Dataframe library\n",
      "\n",
      "\n",
      "\n",
      "import matplotlib.pyplot as plt # Library for plots\n",
      "\n",
      "from utils import confidence_ellipse # Function to add confidence ellipses to charts\n",
      "\n",
      "data = pd.read_csv('./data/bayes_features.csv'); # Load the data from the csv file\n",
      "\n",
      "\n",
      "\n",
      "data.head(5) # Print the first 5 tweets features. Each row represents a tweet\n",
      "\n",
      "# Plot the samples using columns 1 and 2 of the matrix\n",
      "\n",
      "fig, ax = plt.subplots(figsize = (8, 8)) #Create a new figure with a custom size\n",
      "\n",
      "\n",
      "\n",
      "colors = ['red', 'green'] # Define a color palete\n",
      "\n",
      "sentiments = ['negative', 'positive'] \n",
      "\n",
      "\n",
      "\n",
      "index = data.index\n",
      "\n",
      "\n",
      "\n",
      "# Color base on sentiment\n",
      "\n",
      "for sentiment in data.sentiment.unique():\n",
      "\n",
      "    ix = index[data.sentiment == sentiment]\n",
      "\n",
      "    ax.scatter(data.iloc[ix].positive, data.iloc[ix].negative, c=colors[int(sentiment)], s=0.1, marker='*', label=sentiments[int(sentiment)])\n",
      "\n",
      "\n",
      "\n",
      "ax.legend(loc='best')    \n",
      "\n",
      "    \n",
      "\n",
      "# Custom limits for this chart\n",
      "\n",
      "plt.xlim(-250,0)\n",
      "\n",
      "plt.ylim(-250,0)\n",
      "\n",
      "\n",
      "\n",
      "plt.xlabel(\"Positive\") # x-axis label\n",
      "\n",
      "plt.ylabel(\"Negative\") # y-axis label\n",
      "\n",
      "plt.show()\n",
      "\n",
      "# Plot the samples using columns 1 and 2 of the matrix\n",
      "\n",
      "fig, ax = plt.subplots(figsize = (8, 8))\n",
      "\n",
      "\n",
      "\n",
      "colors = ['red', 'green'] # Define a color palete\n",
      "\n",
      "sentiments = ['negative', 'positive'] \n",
      "\n",
      "index = data.index\n",
      "\n",
      "\n",
      "\n",
      "# Color base on sentiment\n",
      "\n",
      "for sentiment in data.sentiment.unique():\n",
      "\n",
      "    ix = index[data.sentiment == sentiment]\n",
      "\n",
      "    ax.scatter(data.iloc[ix].positive, data.iloc[ix].negative, c=colors[int(sentiment)], s=0.1, marker='*', label=sentiments[int(sentiment)])\n",
      "\n",
      "\n",
      "\n",
      "# Custom limits for this chart\n",
      "\n",
      "plt.xlim(-200,40)  \n",
      "\n",
      "plt.ylim(-200,40)\n",
      "\n",
      "\n",
      "\n",
      "plt.xlabel(\"Positive\") # x-axis label\n",
      "\n",
      "plt.ylabel(\"Negative\") # y-axis label\n",
      "\n",
      "\n",
      "\n",
      "data_pos = data[data.sentiment == 1] # Filter only the positive samples\n",
      "\n",
      "data_neg = data[data.sentiment == 0] # Filter only the negative samples\n",
      "\n",
      "\n",
      "\n",
      "# Print confidence ellipses of 2 std\n",
      "\n",
      "confidence_ellipse(data_pos.positive, data_pos.negative, ax, n_std=2, edgecolor='black', label=r'$2\\sigma$' )\n",
      "\n",
      "confidence_ellipse(data_neg.positive, data_neg.negative, ax, n_std=2, edgecolor='orange')\n",
      "\n",
      "\n",
      "\n",
      "# Print confidence ellipses of 3 std\n",
      "\n",
      "confidence_ellipse(data_pos.positive, data_pos.negative, ax, n_std=3, edgecolor='black', linestyle=':', label=r'$3\\sigma$')\n",
      "\n",
      "confidence_ellipse(data_neg.positive, data_neg.negative, ax, n_std=3, edgecolor='orange', linestyle=':')\n",
      "\n",
      "ax.legend(loc='lower right')\n",
      "\n",
      "\n",
      "\n",
      "plt.show()\n",
      "\n",
      "data2 = data.copy() # Copy the whole data frame\n",
      "\n",
      "\n",
      "\n",
      "# The following 2 lines only modify the entries in the data frame where sentiment == 1\n",
      "\n",
      "data2.negative[data.sentiment == 1] =  data2.negative * 1.5 + 50 # Modify the negative attribute\n",
      "\n",
      "data2.positive[data.sentiment == 1] =  data2.positive / 1.5 - 50 # Modify the positive attribute \n",
      "\n",
      "# Plot the samples using columns 1 and 2 of the matrix\n",
      "\n",
      "fig, ax = plt.subplots(figsize = (8, 8))\n",
      "\n",
      "\n",
      "\n",
      "colors = ['red', 'green'] # Define a color palete\n",
      "\n",
      "sentiments = ['negative', 'positive'] \n",
      "\n",
      "index = data2.index\n",
      "\n",
      "\n",
      "\n",
      "# Color base on sentiment\n",
      "\n",
      "for sentiment in data2.sentiment.unique():\n",
      "\n",
      "    ix = index[data2.sentiment == sentiment]\n",
      "\n",
      "    ax.scatter(data2.iloc[ix].positive, data2.iloc[ix].negative, c=colors[int(sentiment)], s=0.1, marker='*', label=sentiments[int(sentiment)])\n",
      "\n",
      "\n",
      "\n",
      "#ax.scatter(data2.positive, data2.negative, c=[colors[int(k)] for k in data2.sentiment], s = 0.1, marker='*')  # Plot a dot for tweet\n",
      "\n",
      "# Custom limits for this chart\n",
      "\n",
      "plt.xlim(-200,40)  \n",
      "\n",
      "plt.ylim(-200,40)\n",
      "\n",
      "\n",
      "\n",
      "plt.xlabel(\"Positive\") # x-axis label\n",
      "\n",
      "plt.ylabel(\"Negative\") # y-axis label\n",
      "\n",
      "\n",
      "\n",
      "data_pos = data2[data2.sentiment == 1] # Filter only the positive samples\n",
      "\n",
      "data_neg = data[data2.sentiment == 0] # Filter only the negative samples\n",
      "\n",
      "\n",
      "\n",
      "# Print confidence ellipses of 2 std\n",
      "\n",
      "confidence_ellipse(data_pos.positive, data_pos.negative, ax, n_std=2, edgecolor='black', label=r'$2\\sigma$' )\n",
      "\n",
      "confidence_ellipse(data_neg.positive, data_neg.negative, ax, n_std=2, edgecolor='orange')\n",
      "\n",
      "\n",
      "\n",
      "# Print confidence ellipses of 3 std\n",
      "\n",
      "confidence_ellipse(data_pos.positive, data_pos.negative, ax, n_std=3, edgecolor='black', linestyle=':', label=r'$3\\sigma$')\n",
      "\n",
      "confidence_ellipse(data_neg.positive, data_neg.negative, ax, n_std=3, edgecolor='orange', linestyle=':')\n",
      "\n",
      "ax.legend(loc='lower right')\n",
      "\n",
      "\n",
      "\n",
      "plt.show()\n",
      "\n",
      "from utils import process_tweet, lookup\n",
      "\n",
      "import pdb\n",
      "\n",
      "from nltk.corpus import stopwords, twitter_samples\n",
      "\n",
      "import numpy as np\n",
      "\n",
      "import pandas as pd\n",
      "\n",
      "import nltk\n",
      "\n",
      "import string\n",
      "\n",
      "from nltk.tokenize import TweetTokenizer\n",
      "\n",
      "from os import getcwd\n",
      "\n",
      "import w2_unittest\n",
      "\n",
      "\n",
      "\n",
      "nltk.download('twitter_samples')\n",
      "\n",
      "nltk.download('stopwords')\n",
      "\n",
      "filePath = f\"{getcwd()}/../tmp2/\"\n",
      "\n",
      "nltk.data.path.append(filePath)\n",
      "\n",
      "# get the sets of positive and negative tweets\n",
      "\n",
      "all_positive_tweets = twitter_samples.strings('positive_tweets.json')\n",
      "\n",
      "all_negative_tweets = twitter_samples.strings('negative_tweets.json')\n",
      "\n",
      "\n",
      "\n",
      "# split the data into two pieces, one for training and one for testing (validation set)\n",
      "\n",
      "test_pos = all_positive_tweets[4000:]\n",
      "\n",
      "train_pos = all_positive_tweets[:4000]\n",
      "\n",
      "test_neg = all_negative_tweets[4000:]\n",
      "\n",
      "train_neg = all_negative_tweets[:4000]\n",
      "\n",
      "\n",
      "\n",
      "train_x = train_pos + train_neg\n",
      "\n",
      "test_x = test_pos + test_neg\n",
      "\n",
      "\n",
      "\n",
      "# avoid assumptions about the length of all_positive_tweets\n",
      "\n",
      "train_y = np.append(np.ones(len(train_pos)), np.zeros(len(train_neg)))\n",
      "\n",
      "test_y = np.append(np.ones(len(test_pos)), np.zeros(len(test_neg)))\n",
      "\n",
      "custom_tweet = \"RT @Twitter @chapagain Hello There! Have a great day. :) #good #morning http://chapagain.com.np\"\n",
      "\n",
      "\n",
      "\n",
      "# print cleaned tweet\n",
      "\n",
      "print(process_tweet(custom_tweet))\n",
      "\n",
      "# UNQ_C1 GRADED FUNCTION: count_tweets\n",
      "\n",
      "\n",
      "\n",
      "def count_tweets(result, tweets, ys):\n",
      "\n",
      "    '''\n",
      "\n",
      "    Input:\n",
      "\n",
      "        result: a dictionary that will be used to map each pair to its frequency\n",
      "\n",
      "        tweets: a list of tweets\n",
      "\n",
      "        ys: a list corresponding to the sentiment of each tweet (either 0 or 1)\n",
      "\n",
      "    Output:\n",
      "\n",
      "        result: a dictionary mapping each pair to its frequency\n",
      "\n",
      "    '''\n",
      "\n",
      "    ### START CODE HERE ###\n",
      "\n",
      "    for y, tweet in zip(ys, tweets):\n",
      "\n",
      "        for word in process_tweet(tweet):\n",
      "\n",
      "            # define the key, which is the word and label tuple\n",
      "\n",
      "            pair = (word, y)\n",
      "\n",
      "            \n",
      "\n",
      "            # if the key exists in the dictionary, increment the count\n",
      "\n",
      "            if pair in result:\n",
      "\n",
      "                result[pair] += 1\n",
      "\n",
      "\n",
      "\n",
      "            # else, if the key is new, add it to the dictionary and set the count to 1\n",
      "\n",
      "            else:\n",
      "\n",
      "                result[pair] = 1\n",
      "\n",
      "    ### END CODE HERE ###\n",
      "\n",
      "\n",
      "\n",
      "    return result\n",
      "\n",
      "# Testing your function\n",
      "\n",
      "\n",
      "\n",
      "result = {}\n",
      "\n",
      "tweets = ['i am happy', 'i am tricked', 'i am sad', 'i am tired', 'i am tired']\n",
      "\n",
      "ys = [1, 0, 0, 0, 0]\n",
      "\n",
      "count_tweets(result, tweets, ys)\n",
      "\n",
      "# Test your function\n",
      "\n",
      "w2_unittest.test_count_tweets(count_tweets)\n",
      "\n",
      "# Build the freqs dictionary for later uses\n",
      "\n",
      "freqs = count_tweets({}, train_x, train_y)\n",
      "\n",
      "# UNQ_C2 GRADED FUNCTION: train_naive_bayes\n",
      "\n",
      "\n",
      "\n",
      "def train_naive_bayes(freqs, train_x, train_y):\n",
      "\n",
      "    '''\n",
      "\n",
      "    Input:\n",
      "\n",
      "        freqs: dictionary from (word, label) to how often the word appears\n",
      "\n",
      "        train_x: a list of tweets\n",
      "\n",
      "        train_y: a list of labels correponding to the tweets (0,1)\n",
      "\n",
      "    Output:\n",
      "\n",
      "        logprior: the log prior. (equation 3 above)\n",
      "\n",
      "        loglikelihood: the log likelihood of you Naive bayes equation. (equation 6 above)\n",
      "\n",
      "    '''\n",
      "\n",
      "    loglikelihood = {}\n",
      "\n",
      "    logprior = 0\n",
      "\n",
      "\n",
      "\n",
      "    ### START CODE HERE ###\n",
      "\n",
      "\n",
      "\n",
      "    # calculate V, the number of unique words in the vocabulary\n",
      "\n",
      "    vocab = [key[0] for key in freqs.keys()]\n",
      "\n",
      "     \n",
      "\n",
      "    V = len(set(vocab))\n",
      "\n",
      "    \n",
      "\n",
      "\n",
      "\n",
      "    # calculate N_pos, N_neg, V_pos, V_neg\n",
      "\n",
      "    N_pos = N_neg = 0\n",
      "\n",
      "    \n",
      "\n",
      "    for pair in freqs.keys():\n",
      "\n",
      "        \n",
      "\n",
      "        # if the label is positive (greater than zero)\n",
      "\n",
      "        if pair[1] > 0:\n",
      "\n",
      "\n",
      "\n",
      "            # Increment the number of positive words by the count for this (word, label) pair\n",
      "\n",
      "           \n",
      "\n",
      "            N_pos += freqs[pair]\n",
      "\n",
      "\n",
      "\n",
      "        # else, the label is negative\n",
      "\n",
      "        else:\n",
      "\n",
      "\n",
      "\n",
      "            # increment the number of negative words by the count for this (word,label) pair\n",
      "\n",
      "            N_neg += freqs[pair]\n",
      "\n",
      "    \n",
      "\n",
      "    # Calculate D, the number of documents\n",
      "\n",
      "    \n",
      "\n",
      "    D = len(train_y)\n",
      "\n",
      "    \n",
      "\n",
      "\n",
      "\n",
      "    # Calculate D_pos, the number of positive documents\n",
      "\n",
      "    D_pos = len(list(filter(lambda x: x == 1, train_y)))\n",
      "\n",
      "    \n",
      "\n",
      "    # Calculate D_neg, the number of negative documents\n",
      "\n",
      "    D_neg = len(list(filter(lambda x: x == 0, train_y)))\n",
      "\n",
      "        \n",
      "\n",
      "    # Calculate logprior\n",
      "\n",
      "    logprior = np.log(D_pos/D_neg)\n",
      "\n",
      "    \n",
      "\n",
      "    \n",
      "\n",
      "    # For each word in the vocabulary...\n",
      "\n",
      "    for word in vocab:\n",
      "\n",
      "        # get the positive and negative frequency of the word\n",
      "\n",
      "        freq_pos = lookup(freqs,word,1)\n",
      "\n",
      "        freq_neg = lookup(freqs,word,0)\n",
      "\n",
      "                        \n",
      "\n",
      "        \n",
      "\n",
      "        # calculate the probability that each word is positive, and negative\n",
      "\n",
      "        p_w_pos = (freq_pos + 1) / (N_pos + V)\n",
      "\n",
      "        p_w_neg = (freq_neg +1) / (N_neg + V)\n",
      "\n",
      "\n",
      "\n",
      "        # calculate the log likelihood of the word\n",
      "\n",
      "        loglikelihood[word] = np.log(p_w_pos/p_w_neg)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "    ### END CODE HERE ###\n",
      "\n",
      "\n",
      "\n",
      "    return logprior, loglikelihood\n",
      "\n",
      "# UNQ_C3 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n",
      "\n",
      "logprior, loglikelihood = train_naive_bayes(freqs, train_x, train_y)\n",
      "\n",
      "print(logprior)\n",
      "\n",
      "print(len(loglikelihood))\n",
      "\n",
      "# Test your function\n",
      "\n",
      "w2_unittest.test_train_naive_bayes(train_naive_bayes, freqs, train_x, train_y)\n",
      "\n",
      "# UNQ_C4 GRADED FUNCTION: naive_bayes_predict\n",
      "\n",
      "\n",
      "\n",
      "def naive_bayes_predict(tweet, logprior, loglikelihood):\n",
      "\n",
      "    '''\n",
      "\n",
      "    Input:\n",
      "\n",
      "        tweet: a string\n",
      "\n",
      "        logprior: a number\n",
      "\n",
      "        loglikelihood: a dictionary of words mapping to numbers\n",
      "\n",
      "    Output:\n",
      "\n",
      "        p: the sum of all the logliklihoods of each word in the tweet (if found in the dictionary) + logprior (a number)\n",
      "\n",
      "\n",
      "\n",
      "    '''\n",
      "\n",
      "    ### START CODE HERE ###\n",
      "\n",
      "    # process the tweet to get a list of words\n",
      "\n",
      "    word_l = process_tweet(tweet)\n",
      "\n",
      "    \n",
      "\n",
      "    # initialize probability to zero\n",
      "\n",
      "    p = 0\n",
      "\n",
      "\n",
      "\n",
      "    # add the logprior\n",
      "\n",
      "    p += logprior\n",
      "\n",
      "\n",
      "\n",
      "    for word in word_l:\n",
      "\n",
      "\n",
      "\n",
      "        # check if the word exists in the loglikelihood dictionary\n",
      "\n",
      "        if word in loglikelihood:\n",
      "\n",
      "            # add the log likelihood of that word to the probability\n",
      "\n",
      "            \n",
      "\n",
      "            p += loglikelihood[word]\n",
      "\n",
      "\n",
      "\n",
      "    ### END CODE HERE ###\n",
      "\n",
      "\n",
      "\n",
      "    return p\n",
      "\n",
      "# UNQ_C5 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n",
      "\n",
      "my_tweet = 'She smiled.'\n",
      "\n",
      "p = naive_bayes_predict(my_tweet, logprior, loglikelihood)\n",
      "\n",
      "print('The expected output is', p)\n",
      "\n",
      "# Test your function\n",
      "\n",
      "w2_unittest.test_naive_bayes_predict(naive_bayes_predict)\n",
      "\n",
      "# Experiment with your own tweet.\n",
      "\n",
      "my_tweet = 'He laughed.'\n",
      "\n",
      "p = naive_bayes_predict(my_tweet, logprior, loglikelihood)\n",
      "\n",
      "print('The expected output is', p)\n",
      "\n",
      "# UNQ_C6 GRADED FUNCTION: test_naive_bayes\n",
      "\n",
      "\n",
      "\n",
      "def test_naive_bayes(test_x, test_y, logprior, loglikelihood, naive_bayes_predict=naive_bayes_predict):\n",
      "\n",
      "    \"\"\"\n",
      "\n",
      "    Input:\n",
      "\n",
      "        test_x: A list of tweets\n",
      "\n",
      "        test_y: the corresponding labels for the list of tweets\n",
      "\n",
      "        logprior: the logprior\n",
      "\n",
      "        loglikelihood: a dictionary with the loglikelihoods for each word\n",
      "\n",
      "    Output:\n",
      "\n",
      "        accuracy: (# of tweets classified correctly)/(total # of tweets)\n",
      "\n",
      "    \"\"\"\n",
      "\n",
      "    accuracy = 0  # return this properly\n",
      "\n",
      "\n",
      "\n",
      "    ### START CODE HERE ###\n",
      "\n",
      "    y_hats = []\n",
      "\n",
      "    for tweet in test_x:\n",
      "\n",
      "        # if the prediction is > 0\n",
      "\n",
      "        if naive_bayes_predict(tweet, logprior, loglikelihood) > 0:\n",
      "\n",
      "            # the predicted class is 1\n",
      "\n",
      "            y_hat_i = 1\n",
      "\n",
      "        else:\n",
      "\n",
      "            # otherwise the predicted class is 0\n",
      "\n",
      "            y_hat_i = 0\n",
      "\n",
      "\n",
      "\n",
      "        # append the predicted class to the list y_hats\n",
      "\n",
      "        y_hats.append(y_hat_i)\n",
      "\n",
      "    \n",
      "\n",
      "    # error is the average of the absolute values of the differences between y_hats and test_y\n",
      "\n",
      "    error = np.mean(abs(y_hats - test_y))\n",
      "\n",
      "    \n",
      "\n",
      "    # Accuracy is 1 minus the error\n",
      "\n",
      "    accuracy = 1 - error\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "    ### END CODE HERE ###\n",
      "\n",
      "\n",
      "\n",
      "    return accuracy\n",
      "\n",
      "print(\"Naive Bayes accuracy = %0.4f\" %\n",
      "\n",
      "      (test_naive_bayes(test_x, test_y, logprior, loglikelihood)))\n",
      "\n",
      "# UNQ_C7 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n",
      "\n",
      "# Run this cell to test your function\n",
      "\n",
      "for tweet in ['I am happy', 'I am bad', 'this movie should have been great.', 'great', 'great great', 'great great great', 'great great great great']:\n",
      "\n",
      "    # print( '%s -> %f' % (tweet, naive_bayes_predict(tweet, logprior, loglikelihood)))\n",
      "\n",
      "    p = naive_bayes_predict(tweet, logprior, loglikelihood)\n",
      "\n",
      "#     print(f'{tweet} -> {p:.2f} ({p_category})')\n",
      "\n",
      "    print(f'{tweet} -> {p:.2f}')\n",
      "\n",
      "# Feel free to check the sentiment of your own tweet below\n",
      "\n",
      "my_tweet = 'you are bad :('\n",
      "\n",
      "naive_bayes_predict(my_tweet, logprior, loglikelihood)\n",
      "\n",
      "# Test your function\n",
      "\n",
      "w2_unittest.unittest_test_naive_bayes(test_naive_bayes, test_x, test_y)\n",
      "\n",
      "# UNQ_C8 GRADED FUNCTION: get_ratio\n",
      "\n",
      "\n",
      "\n",
      "def get_ratio(freqs, word):\n",
      "\n",
      "    '''\n",
      "\n",
      "    Input:\n",
      "\n",
      "        freqs: dictionary containing the words\n",
      "\n",
      "\n",
      "\n",
      "    Output: a dictionary with keys 'positive', 'negative', and 'ratio'.\n",
      "\n",
      "        Example: {'positive': 10, 'negative': 20, 'ratio': 0.5}\n",
      "\n",
      "    '''\n",
      "\n",
      "    pos_neg_ratio = {'positive': 0, 'negative': 0, 'ratio': 0.0}\n",
      "\n",
      "    ### START CODE HERE ###\n",
      "\n",
      "    # use lookup() to find positive counts for the word (denoted by the integer 1)\n",
      "\n",
      "    pos_neg_ratio['positive'] = lookup(freqs, word, 1)\n",
      "\n",
      "    \n",
      "\n",
      "    # use lookup() to find negative counts for the word (denoted by integer 0)\n",
      "\n",
      "    pos_neg_ratio['negative'] = lookup(freqs, word, 0)\n",
      "\n",
      "    \n",
      "\n",
      "    # calculate the ratio of positive to negative counts for the word\n",
      "\n",
      "    pos_neg_ratio['ratio'] = (pos_neg_ratio['positive'] + 1) / (pos_neg_ratio['negative'] +1)\n",
      "\n",
      "    ### END CODE HERE ###\n",
      "\n",
      "    return pos_neg_ratio\n",
      "\n",
      "\n",
      "get_ratio(freqs, 'happi')\n",
      "\n",
      "# Test your function\n",
      "\n",
      "w2_unittest.test_get_ratio(get_ratio, freqs)\n",
      "\n",
      "# UNQ_C9 GRADED FUNCTION: get_words_by_threshold\n",
      "\n",
      "\n",
      "\n",
      "def get_words_by_threshold(freqs, label, threshold, get_ratio=get_ratio):\n",
      "\n",
      "    '''\n",
      "\n",
      "    Input:\n",
      "\n",
      "        freqs: dictionary of words\n",
      "\n",
      "        label: 1 for positive, 0 for negative\n",
      "\n",
      "        threshold: ratio that will be used as the cutoff for including a word in the returned dictionary\n",
      "\n",
      "    Output:\n",
      "\n",
      "        word_list: dictionary containing the word and information on its positive count, negative count, and ratio of positive to negative counts.\n",
      "\n",
      "        example of a key value pair:\n",
      "\n",
      "        {'happi':\n",
      "\n",
      "            {'positive': 10, 'negative': 20, 'ratio': 0.5}\n",
      "\n",
      "        }\n",
      "\n",
      "    '''\n",
      "\n",
      "    word_list = {}\n",
      "\n",
      "\n",
      "\n",
      "    ### START CODE HERE ###\n",
      "\n",
      "    for key in freqs.keys():\n",
      "\n",
      "        word, _ = key\n",
      "\n",
      "\n",
      "\n",
      "        # get the positive/negative ratio for a word\n",
      "\n",
      "        pos_neg_ratio = get_ratio(freqs, word)\n",
      "\n",
      "\n",
      "\n",
      "        # if the label is 1 and the ratio is greater than or equal to the threshold...\n",
      "\n",
      "        if label == 1 and pos_neg_ratio['ratio'] >= threshold:\n",
      "\n",
      "        \n",
      "\n",
      "            # Add the pos_neg_ratio to the dictionary\n",
      "\n",
      "            word_list[word] = pos_neg_ratio\n",
      "\n",
      "\n",
      "\n",
      "        # If the label is 0 and the pos_neg_ratio is less than or equal to the threshold...\n",
      "\n",
      "        elif label == 0 and pos_neg_ratio['ratio'] <= threshold:\n",
      "\n",
      "        \n",
      "\n",
      "            # Add the pos_neg_ratio to the dictionary\n",
      "\n",
      "            word_list[word] = pos_neg_ratio\n",
      "\n",
      "\n",
      "\n",
      "        # otherwise, do not include this word in the list (do nothing)\n",
      "\n",
      "\n",
      "\n",
      "    ### END CODE HERE ###\n",
      "\n",
      "    return word_list\n",
      "\n",
      "\n",
      "# Test your function: find negative words at or below a threshold\n",
      "\n",
      "get_words_by_threshold(freqs, label=0, threshold=0.05)\n",
      "\n",
      "# Test your function; find positive words at or above a threshold\n",
      "\n",
      "get_words_by_threshold(freqs, label=1, threshold=10)\n",
      "\n",
      "# Test your function\n",
      "\n",
      "w2_unittest.test_get_words_by_threshold(get_words_by_threshold, freqs)\n",
      "\n",
      "# Some error analysis done for you\n",
      "\n",
      "print('Truth Predicted Tweet')\n",
      "\n",
      "for x, y in zip(test_x, test_y):\n",
      "\n",
      "    y_hat = naive_bayes_predict(x, logprior, loglikelihood)\n",
      "\n",
      "    if y != (np.sign(y_hat) > 0):\n",
      "\n",
      "        print('%d\\t%0.2f\\t%s' % (y, np.sign(y_hat) > 0, ' '.join(\n",
      "\n",
      "            process_tweet(x)).encode('ascii', 'ignore')))\n",
      "\n",
      "# Test with your own tweet - feel free to modify `my_tweet`\n",
      "\n",
      "my_tweet = 'I am happy because I am learning :)'\n",
      "\n",
      "\n",
      "\n",
      "p = naive_bayes_predict(my_tweet, logprior, loglikelihood)\n",
      "\n",
      "print(p)\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Load the JSON file\n",
    "with open(\"repo_data.json\", \"r\", encoding=\"utf-8\") as json_file:\n",
    "    repo_data = json.load(json_file)\n",
    "\n",
    "# Extract code from .ipynb files\n",
    "all_code_snippets = []\n",
    "for file_path, content in repo_data.items():\n",
    "    if file_path.endswith(\".ipynb\") and content:\n",
    "        try:\n",
    "            notebook_data = json.loads(content)  # Parse JSON\n",
    "            for cell in notebook_data.get(\"cells\", []):\n",
    "                if cell[\"cell_type\"] == \"code\":\n",
    "                    all_code_snippets.append(\"\\n\".join(cell[\"source\"]))\n",
    "        except json.JSONDecodeError:\n",
    "            print(f\"Error parsing {file_path}\")\n",
    "\n",
    "# Merge extracted code into a single text block\n",
    "full_code = \"\\n\\n\".join(all_code_snippets)\n",
    "\n",
    "# Print the extracted code (Optional)\n",
    "print(full_code)\n",
    "\n",
    "# Now you can send `full_code` to Gemini API for summarization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This Python code performs sentiment analysis on tweets using two different methods: Logistic Regression and Naive Bayes. Here's a breakdown of its functionality:\n",
      "\n",
      "**1. Data Preparation:**\n",
      "\n",
      "*   Downloads necessary NLTK datasets (twitter\\_samples, stopwords).\n",
      "*   Loads positive and negative tweets from the `twitter_samples` corpus.\n",
      "*   Splits the data into training and testing sets (80% training, 20% testing).\n",
      "*   Creates NumPy arrays for the training and testing labels (1 for positive, 0 for negative).\n",
      "\n",
      "**2. Logistic Regression Implementation:**\n",
      "\n",
      "*   **`process_tweet(tweet)`:** (Imported from tweetPrepro\\_freqGenerator.py) Cleans and preprocesses the tweet text by tokenizing, stemming, and removing stop words.\n",
      "*   **`build_freqs(tweets, labels)`:** (Imported from tweetPrepro\\_freqGenerator.py) Creates a frequency dictionary that counts how often each word appears with a positive or negative label.\n",
      "*   **`sigmoid(z)`:**  Implements the sigmoid function, which maps any input to a value between 0 and 1 (representing probability).\n",
      "*   **`gradientDescent(x, y, theta, alpha, num_iters)`:**  Implements gradient descent to learn the optimal weights (theta) for the logistic regression model.  It minimizes the cost function by iteratively updating the weights based on the error between predictions and actual labels.\n",
      "*   **`extract_features(tweet, freqs)`:**  Extracts features from a tweet based on the frequency dictionary.  It creates a feature vector of size (1,3):  \\[1, sum\\_positive\\_word\\_counts, sum\\_negative\\_word\\_counts]. The '1' is a bias term.\n",
      "*   **Training the Logistic Regression Model:**\n",
      "    *   Extracts features (X) and labels (Y) from the training data.\n",
      "    *   Trains the logistic regression model using the `gradientDescent` function, obtaining the final cost and weight vector.\n",
      "*   **`predict_tweet(tweet, freqs, theta)`:** Predicts the sentiment of a tweet using the trained logistic regression model. It extracts features from the tweet, calculates the probability using the sigmoid function, and returns the probability of the tweet being positive.\n",
      "*   **`test_logistic_regression(test_x, test_y, freqs, theta)`:** Evaluates the logistic regression model on the test data. It predicts the sentiment of each tweet, compares it to the true label, and calculates the accuracy of the model.\n",
      "*   **Error Analysis:**  Prints out examples of tweets that were misclassified by the logistic regression model.\n",
      "*   **Visualization:** Uses the data from `./data/bayes_features.csv` to plot the tweets in a 2D space where the x and y axis are positive and negative counts. It shows the confidence ellipses of the points.\n",
      "\n",
      "**3. Naive Bayes Implementation:**\n",
      "\n",
      "*   **`count_tweets(result, tweets, ys)`:** Counts the occurrences of each word in positive and negative tweets and stores them in a dictionary.  This creates the frequency dictionary for Naive Bayes.\n",
      "*   **`train_naive_bayes(freqs, train_x, train_y)`:** Trains the Naive Bayes classifier. It calculates the log prior (probability of a tweet being positive or negative) and the log likelihood (probability of a word given the sentiment).\n",
      "*   **`naive_bayes_predict(tweet, logprior, loglikelihood)`:** Predicts the sentiment of a tweet using the trained Naive Bayes classifier. It calculates the probability by summing the log likelihoods of the words in the tweet and adding the log prior.\n",
      "*   **`test_naive_bayes(test_x, test_y, logprior, loglikelihood)`:** Evaluates the Naive Bayes classifier on the test data, calculating the accuracy.\n",
      "*   **`get_ratio(freqs, word)`:** Calculates the ratio of positive to negative counts for a given word.\n",
      "*   **`get_words_by_threshold(freqs, label, threshold)`:** Returns a dictionary of words that meet a specified threshold for their positive/negative ratio.  This is useful for identifying words that are strongly associated with either positive or negative sentiment.\n",
      "*   **Error Analysis:** Prints examples of tweets that were misclassified by the Naive Bayes classifier.\n",
      "\n",
      "**In summary, the code demonstrates two different approaches to sentiment analysis, Logistic Regression and Naive Bayes, using the Twitter dataset. It includes data preprocessing, model training, prediction, evaluation, and error analysis.**\n",
      "\n",
      "**Sample Test Values for Unit Testing:**\n",
      "\n",
      "Here are 4 sample test values you could use for unit testing, along with what they are testing:\n",
      "\n",
      "1.  **Testing `sigmoid` function with edge case:**\n",
      "    *   Input: `z = 100`\n",
      "    *   Expected Output: very close to 1.0 (e.g., `0.9999999999...`)\n",
      "\n",
      "2.  **Testing `extract_features` with an empty tweet:**\n",
      "    *   Input: `tweet = \"\"`, `freqs` (a sample frequency dictionary)\n",
      "    *   Expected Output: `np.array([[1., 0., 0.]])`  (Only the bias term should be present).\n",
      "\n",
      "3.  **Testing `naive_bayes_predict` with a neutral tweet and zero loglikelihoods:**\n",
      "    *   Input: `tweet = \"the\"` (common word), `logprior = 0`, `loglikelihood = {}` (empty loglikelihood)\n",
      "    *   Expected Output: `0`\n",
      "\n",
      "4.  **Testing `test_logistic_regression` with a perfect model:**\n",
      "    *   Input: `test_x = [\"good\", \"bad\"]`, `test_y = np.array([[1], [0]])`, `freqs`, `theta = np.array([[1], [1], [-1]])` (designed to perfectly classify)\n",
      "    *   Expected Output: `1.0` (100% accuracy)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import google.generativeai as genai\n",
    "\n",
    "# Configure API Key\n",
    "genai.configure(api_key=\"\")\n",
    "\n",
    "# Select a model (e.g., \"gemini-pro\")\n",
    "model = genai.GenerativeModel(\"gemini-2.0-flash\")\n",
    "\n",
    "# Generate a response\n",
    "response = model.generate_content(\n",
    "    f\"Summarize the functionality of this Python code snippet:\\n\\n```{full_code}```\\n\\n also generate 4 sample test values for unit testing the codes \"\n",
    ")\n",
    "\n",
    "# Print the response\n",
    "print(response.text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
